{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learn-programmers/programmers_kdt_II/blob/main/9%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqhTDfuWrcM"
      },
      "source": [
        "PySpark을 로컬머신에 설치하고 노트북을 사용하기 보다는 머신러닝 관련 다양한 라이브러리가 이미 설치되었고 좋은 하드웨어를 제공해주는 Google Colab을 통해 실습을 진행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIA23YgbXKJd"
      },
      "source": [
        "이를 위해 pyspark과 Py4J 패키지를 설치한다. Py4J 패키지는 파이썬 프로그램이 자바가상머신상의 오브젝트들을 접근할 수 있게 해준다. Local Standalone Spark을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbT0rpGfVdiq",
        "outputId": "32661d29-d279-4542-b6b4-a294c676da5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612229 sha256=0984c47b1ab15c78e69eb3035ad630491c8b2b9098f561260ae5dbcd6126580c\n",
            "  Stored in directory: /Users/jykim/Library/Caches/pip/wheels/19/b0/c8/6cb894117070e130fc44352c2a13f15b6c27e440d04a84fb48\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQIB8vLPBMeu",
        "outputId": "058ae550-0d54-4825-ebf9-06f761184805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 55856\n",
            "-rw-r--r--  1 jykim  staff     45706 Nov 11 16:04 3일차_SQL_실습.ipynb\n",
            "-rw-r--r--  1 jykim  staff  21537493 Nov 11 16:04 2일차_SQL_실습.ipynb\n",
            "-rw-r--r--  1 jykim  staff    116585 Nov 11 16:04 4일차_실습.ipynb\n",
            "-rw-r--r--  1 jykim  staff   1082847 Nov 11 16:02 5일차_실습.ipynb\n",
            "-rw-r--r--  1 jykim  staff   2656818 Nov 11 15:49 SQL_JOIN.ipynb\n",
            "-rw-r--r--  1 jykim  staff    449040 Nov 11 15:49 SQL_last.ipynb\n",
            "-rw-r--r--  1 jykim  staff    521170 Nov  9 23:54 SQL_GROUP_BY_CTAS.ipynb\n",
            "-rw-r--r--  1 jykim  staff         0 Nov  9 19:02 라이브세션-2.ipynb\n",
            "-rw-r--r--  1 jykim  staff   1930247 Nov  9 04:16 SQL_analysis.ipynb\n",
            "-rw-r--r--  1 jykim  staff     53662 Nov  7 15:08 11주차_SQL_실습_3일차_1.ipynb\n",
            "-rw-r--r--  1 jykim  staff     65835 Nov  7 15:08 11주차_PySpark_기본_5일차_1.ipynb\n",
            "-rw-r--r--  1 jykim  staff     30077 Nov  7 15:08 11주차_PySpark_기본_4일차_2.ipynb\n",
            "-rw-r--r--  1 jykim  staff     32231 Nov  7 15:08 11주차_PySpark_기본_4일차_1.ipynb\n",
            "-rw-r--r--  1 jykim  staff     23102 Nov  7 15:08 11주차_PySpark_기본_3일차_2.ipynb\n",
            "-rw-r--r--  1 jykim  staff     17015 Nov  7 15:08 11주차_PySpark_기본_2일차.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls -tl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHXY3xYctlCe",
        "outputId": "0441f11c-dc75-43e6-c66e-512bedfdf008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: sample_data: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls -tl sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_eTGrvXlDw"
      },
      "source": [
        "**Spark Session:** SparkSession은 Spark 2.0부터 엔트리 포인트로 사용된다. 그 이전에는 SparkContext가 사용되었다. SparkSession을 이용해 RDD, 데이터 프레임등을 만든다. SparkSession은 SparkSession.builder를 호출하여 생성하며 다양한 함수들을 통해 세부 설정이 가능하다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3vm6tgcPXdnR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/11/11 16:08:50 WARN Utils: Your hostname, KimJY.local resolves to a loopback address: 127.0.0.1; using 192.168.0.23 instead (on interface en0)\n",
            "23/11/11 16:08:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Exception in thread \"main\" java.lang.ExceptionInInitializerError\n",
            "\tat org.apache.spark.unsafe.array.ByteArrayMethods.<clinit>(ByteArrayMethods.java:54)\n",
            "\tat org.apache.spark.internal.config.package$.<init>(package.scala:1006)\n",
            "\tat org.apache.spark.internal.config.package$.<clinit>(package.scala)\n",
            "\tat org.apache.spark.deploy.SparkSubmitArguments.$anonfun$loadEnvironmentArguments$3(SparkSubmitArguments.scala:157)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.deploy.SparkSubmitArguments.loadEnvironmentArguments(SparkSubmitArguments.scala:157)\n",
            "\tat org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:115)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:990)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:990)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
            "Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make private java.nio.DirectByteBuffer(long,int) accessible: module java.base does not \"opens java.nio\" to unnamed module @3f025b6b\n",
            "\tat java.base/java.lang.reflect.AccessibleObject.throwInaccessibleObjectException(AccessibleObject.java:387)\n",
            "\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:363)\n",
            "\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:311)\n",
            "\tat java.base/java.lang.reflect.Constructor.checkCanSetAccessible(Constructor.java:192)\n",
            "\tat java.base/java.lang.reflect.Constructor.setAccessible(Constructor.java:185)\n",
            "\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:56)\n",
            "\t... 13 more\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "Java gateway process exited before sending its port number",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11주차_PySpark_기본_2일차.ipynb 셀 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mPySpark_Tutorial\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/DevCourse_6/lib/python3.10/site-packages/pyspark/sql/session.py:186\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    185\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    187\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/DevCourse_6/lib/python3.10/site-packages/pyspark/context.py:376\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    375\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    377\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/DevCourse_6/lib/python3.10/site-packages/pyspark/context.py:133\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    136\u001b[0m                   conf, jsc, profiler_cls)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/DevCourse_6/lib/python3.10/site-packages/pyspark/context.py:325\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 325\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    326\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/DevCourse_6/lib/python3.10/site-packages/pyspark/java_gateway.py:105\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[1;32m    108\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
            "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName('PySpark_Tutorial')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "LSs_1PYaYWxI",
        "outputId": "0f2ac99a-c9f5-4524-d9cb-8ea24e94f627"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11주차_PySpark_기본_2일차.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jykim/myWS/Dev_AI_6/AI_DevCourse_6/11w/11%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNHAAJuCZCqE"
      },
      "source": [
        "**Python 객체를 RDD로 변환해보기**\n",
        "\n",
        "**1> Python 리스트 생성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhijTd1UY1i9"
      },
      "outputs": [],
      "source": [
        "name_list_json = [ '{\"name\": \"keeyong\"}', '{\"name\": \"benjamin\"}', '{\"name\": \"claire\"}' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXqf1lC_Zdxf",
        "outputId": "553d372c-ea45-4b8a-f5ed-61c8032e9308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"name\": \"keeyong\"}\n",
            "{\"name\": \"benjamin\"}\n",
            "{\"name\": \"claire\"}\n"
          ]
        }
      ],
      "source": [
        "for n in name_list_json:\n",
        "  print(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1_A6f6Nbr57",
        "outputId": "d4339f51-e7e6-483b-c13d-5091023918ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "keeyong\n",
            "benjamin\n",
            "claire\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "for n in name_list_json:\n",
        "  jn = json.loads(n)\n",
        "  print(jn[\"name\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKlanTznb75N"
      },
      "source": [
        "**2> 파이썬 리스트를 RDD로 변환. RDD로 변환되는 순간 Spark 클러스터의 서버들에 데이터가 나눠 저장됨 (파티션). 또한 Lazy Execution이 된다는 점 기억**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvTOpLsrZ_I8"
      },
      "outputs": [],
      "source": [
        "rdd = spark.sparkContext.parallelize(name_list_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIMTLkPdk4ul",
        "outputId": "1bea447d-f88b-43f3-b262-77c8263c615a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmszVYtzaL3Q",
        "outputId": "2d9d3f51-142f-4b95-f931-a60d4da466f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMLop2SQbMnZ"
      },
      "outputs": [],
      "source": [
        "parsed_rdd = rdd.map(lambda el:json.loads(el))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAgNR7EWcl2t",
        "outputId": "a7bc7b08-9c96-46ab-fe9c-d3674dafc398"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[6] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 30,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt5SMf4IcoIZ",
        "outputId": "230bea4b-cdc5-41c5-eda1-b8fe87cba814"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'keeyong'}, {'name': 'benjamin'}, {'name': 'claire'}]"
            ]
          },
          "execution_count": 31,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1Kmn_qycqQl"
      },
      "outputs": [],
      "source": [
        "parsed_name_rdd = rdd.map(lambda el:json.loads(el)[\"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZWxTKgJnDm8",
        "outputId": "f10bca80-db64-4055-bd1f-bbab948147b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['keeyong', 'benjamin', 'claire']"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_name_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McGKfLtWnsTB"
      },
      "source": [
        "**파이썬 리스트를 데이터프레임으로 변환하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WlhiRsKo7j7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df = spark.createDataFrame(name_list_json, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2-pyiKGpLZe",
        "outputId": "04fe945a-2395-4f6f-ed38-a4266a04f584"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 36,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sql4KPhppgr",
        "outputId": "d9c659e8-193a-4a64-e13b-cfe7d6494b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2XzTjdEpuZ4",
        "outputId": "5145ec97-4851-4e72-ac8f-4f2921d65064"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(value='{\"name\": \"keeyong\"}'),\n",
              " Row(value='{\"name\": \"benjamin\"}'),\n",
              " Row(value='{\"name\": \"claire\"}')]"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select('*').collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmLv4Guwny00",
        "outputId": "5072cc74-6e70-4651-c6aa-451b6a3c86c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(value='{\"name\": \"keeyong\"}'),\n",
              " Row(value='{\"name\": \"benjamin\"}'),\n",
              " Row(value='{\"name\": \"claire\"}')]"
            ]
          },
          "execution_count": 39,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select('value').collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93TLUFxgqLrm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "row = Row(\"name\") # Or some other column name\n",
        "df_name = parsed_name_rdd.map(row).toDF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFEYRG61-L8",
        "outputId": "ceea17ca-9524-459b-f35a-e6805230d2da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_name.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MptOIByKsLeG",
        "outputId": "476cbc99-a4e8-4b85-88d4-a346def6bd53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(name='keeyong'), Row(name='benjamin'), Row(name='claire')]"
            ]
          },
          "execution_count": 41,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_name.select('name').collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "PySpark 기본: 2일차",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
