{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계학습과 수학 \n",
    "\n",
    "# 선형대수학\n",
    "\n",
    "- 수학 : 목적함수 정의, 목적함수의 최저점을 찾아주는 최적화 이론 제공\n",
    "- 최적화 이론에 학습률, epoch stop조건 설정함.\n",
    "- 기계학습을 이해하기 위한 관련된 **기본 선형대수**를 확인하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 \n",
    "- 샘플을 특징 벡터로 표현함.\n",
    "- ex. Iris 데이터에서 꽃받침의 길이, 너비, 꽃잎 길이, 너비라는 4개의 특징이 각각 (5.1,3.0,2.1,0.5)인 벡터\n",
    "\n",
    "- 요소의 종류와 크기 표현\n",
    "    - $x \\in R^n$\n",
    "    - $x_1 = \\begin{pmatrix}5.1 \\\\ 3.0 \\\\ 2.1 \\\\ 0.5 \\end{pmatrix}$\n",
    "    - 즉, 특정 공간의 **점**\n",
    "\n",
    "## 행렬 \n",
    "- 여러개의 벡터를 담음.\n",
    "- 훈련집합을 담은 행렬을 **설계 행렬**이라 부른다.\n",
    "- ex. $ X = \\begin{pmatrix}5.1 & 3.0 & 2.1 & 0.5 \\\\ ... & ... & ... & ... \\\\\n",
    "\\end{pmatrix}$ \n",
    "    - 150개의 data라면 이와같이 $150 \\times 4$ 행렬 형태를 이룸\n",
    "\n",
    "### 전치행렬\n",
    "- $A \\to A^T$\n",
    "- 원래의 행벡터들을 열벡터로 바꾼다고 생각할 수 있음.\n",
    "- $A_{i,j}^T = A_{j,i}$\n",
    "- $(AB)^T = B^TA^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활용\n",
    "- 행렬을 이용하면 방정식(선형 방정식)을 간결히 표현 가능하다 $\\to$ 선형시스템(Linear System)\n",
    "\n",
    "- 특수행렬\n",
    "    - 정사각행렬\n",
    "    - 대각행렬\n",
    "    - 단위행렬\n",
    "    - 대칭행렬\n",
    "\n",
    "- 선형대수 : 공간변환 ($Ax=b$) - 각 특수행렬별 공간변화적인 의미도 존재하므로 알아두기.\n",
    "\n",
    "### 행렬 연산a\n",
    "- 행렬 곱셈 : 공간의 변환을 야기함.\n",
    "    - 교환 법척 성립 x : $AB ≠ BA$\n",
    "    - 분배, 결합 법칙 성립 \n",
    "    \n",
    "    $A(B+C) = AB+AC, A(BC) = (AB)C $\n",
    "    - 행렬의 덧셈 : 공간을 옮김.\n",
    "\n",
    "- 벡터의 내적 : 벡터의 유사도(두 벡터가 얼마나 유사한지)\n",
    "    - $\\vec{a} \\cdot \\vec{b} > 0$ : 두 벡터가 예각을 이룸 - 같은 방향\n",
    "    -  $\\vec{a} \\cdot \\vec{b} < 0$ : 두 벡터가 둔각을 이룸 - 다른 방향\n",
    "    - $\\vec{a} \\cdot \\vec{b} = 0$ : 두 벡터 90도를 이룸\n",
    "    \n",
    "\n",
    "- 행렬 곱셈을 통한 벡터의 변환\n",
    "![Alt text](./img/image-7.png)\n",
    "$Ax=b$에서, $x = \\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}$(4차원 공간의 벡터)이 행렬$A$를 통해 $b = \\begin{pmatrix}5\\\\8\\end{pmatrix}$(2차원 벡터)로 변환됨\n",
    "\n",
    "### 텐서\n",
    "- 3차원 이상의 구조를 가진 배열\n",
    "    - 0차 : scalar\n",
    "    - 1차 : vector\n",
    "    - 2차 : 행렬\n",
    "    - 고차원 : Tensor\n",
    "    ![Alt text](./img/image-8.png)\n",
    "\n",
    "### 유사도와 거리 \n",
    "- 벡터의 기하학적 해석\n",
    "- 유사도 : 벡터 사이 **각도**로 판단\n",
    "- 거리 : 유클리드, 맨해튼\n",
    "- 코사인 유사도\n",
    "\n",
    "    $cosine_similarity(a,b) = \\frac{a}{||a||} \\cdot \\frac{b}{||b||} = cos\\theta$\n",
    "\n",
    "### Norm\n",
    "- 벡터와 행렬의 거리(크기)를 norm으로 측정\n",
    "    - p차 norm : $(\\displaystyle\\sum_{i=1,d}|x|^p)^{\\frac{1}{p}}$\n",
    "    - 물리적 의미 : 1차원(1차원벡터가 1인길이 : 마름모, 2차원 벡터가 1인 길이 : 원, 3차원 : 구)\n",
    "![Alt text](./img/image-9.png)\n",
    "\n",
    "    \n",
    "- 행렬의 프로베니우스 norm : 행렬의 크기 측정 - 행렬 모든 원소 제곱을 더하여 루트 씌움\n",
    "\n",
    "###  벡터 공간\n",
    "- 기저 벡터들의 선형결합이 만드는 벡터공간 \n",
    "\n",
    "### 역행렬\n",
    "벡터를 **변환 이전**의 공간의 벡터로 돌려줌\n",
    "- 역행렬을 활용한 방정식의 표현과 해\n",
    "- $Ax=b \\to x=A^{-1}b$ \n",
    "\n",
    "### 행렬식\n",
    "- 기하학적 의미 : 행렬의 곱에 의한 공간의 **확장 또는 축소** 해석\n",
    "    - det($A$) = 0, 하나의 차원을 따라 축소되어 부피를 잃음. (역행렬 x)\n",
    "    - det($A$) = 1, 부피를 유지한 변환, 방향 보존\n",
    "    - det($A$) = -1, 부피 유지, 방향 보존 안됨.\n",
    "    - det($A$) = 5, 5배 부피 확장되어 방향 보존.\n",
    "\n",
    "![Alt text](./img/image-10.png)\n",
    "\n",
    "### 정리 : 서로 필요충분조건\n",
    "- A는 역행렬을 가진다.\n",
    "- A는 최대 Rank를 가진다.\n",
    "- A의 모든 행이 선형 독립.\n",
    "- A의 모든 열이 선형 독립.\n",
    "- A의 행렬식 0이 아니다.\n",
    "- $A^tA$는 양의 **정부호**(positive definite) 대칭 행렬이다.\n",
    "- $A$의 고윳값은 모두 0이 아니다.\n",
    "\n",
    "### 정부호 행렬\n",
    " 양의 정부호 행렬 : 0이 아닌 모든 벡터 x에 대해, $x^TAx>0$\n",
    "- 성질\n",
    "    - 고윳값 모두 양수\n",
    "    - 역행렬 또한 정부호행렬\n",
    "    - 행렬식이 0이 아닌 $A$가 존재한다.\n",
    "\n",
    "- 종류\n",
    "    - 양의 준정부호 행렬 : 0이 아닌 모든 벡터 x에 대해, $x^TAx≥0$\n",
    "    - 음의 정부호 행렬 : 0이 아닌 모든 벡터 x에 대해, $x^TAx<0$\n",
    "    - 음의 준정부호 행렬 : 0이 아닌 모든 벡터 x에 대해, $x^TAx≤0$\n",
    "\n",
    "### 분해\n",
    "정수 $\\to$ 소인수분해\n",
    "-> 행렬을 분해함.\n",
    "- 고유치 고유벡터\n",
    "    - $Av=\\lambda v$\n",
    "- 효과\n",
    "![Alt text](./img/image-11.png)\n",
    "- left : 원으로 표현된 단위벡터 $u\\in R^2 $\n",
    "- right : 행렬 $A$의 곱에 의한 $Au$ 모든 점\n",
    "- $A$가 원을 고유벡터 방향에 대해 고유값 크기만큼 scalar배하여 변환\n",
    "\n",
    "### 고유 분해 : eigen-decomposition\n",
    "- $A=QAQ^{-1}$\n",
    "- $Q$는 $A$의 고유벡터를 열에 배치한 행렬, $A$는 고윳값을 대각선에 배치한 대각행렬\n",
    "- 이는 고유치 고유벡터가 존재하는 **정사각행렬**에만 적용 가능\n",
    "\n",
    "### 특이값 분해 : SVD\n",
    "$A = U$∑$V^T$\n",
    "    - $U$ : $AA^T$의 고유벡터를 열에 배치한 $n\\times n$행렬\n",
    "    - $V$ : $A^TA$의 고유벡터를 열에 배치한 $m\\times m$행렬\n",
    "    - ∑ : $AA^T$의 고윳값의 제곱근을 대각선에 배치한 $n\\times m$ 대각행렬\n",
    "\n",
    "![Alt text](./img/image-12.png)\n",
    "- 기하학적 해석\n",
    "![Alt text](./img/image-13.png)\n",
    "\n",
    "- 특잇값 분해 : 정사각행렬이 **아닌 행렬**의 역행렬 계산에 사용\n",
    "![Alt text](./img/image-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률과 통계\n",
    "\n",
    "- 기계학습 시, 처리할 데이터에 불확실성이 존재하므로, 이를 다루는 확률과 통계 활용\n",
    "- 원하는 결과가 언제 어떻게 나오는 규칙을 파악하고자 함 ( 가설 혹은 모델 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 확률 변수\n",
    "\n",
    "- 특정 상황에 대한 경우의 수 \n",
    "    - ex. 윷놀이 : 도,개,걸,윷,모 - 각 5가지의 확률 변수를 가짐\n",
    "\n",
    "#### 확률 분포\n",
    "- 확률질량함수 : PMF - **이산확률변수**\n",
    "    - ex. $P(x=도) = \\frac{4}{16}$\n",
    "- 확률밀도함수 : PDF - **연속확률변수**\n",
    "\n",
    "#### 확률벡터\n",
    "- 확률변수를 요소로 가짐.\n",
    "    - ex. Iris에서 4개의 x특성\n",
    "    - $x$ = $(x_1,x_2,x_3,x_4)^T$ : (꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비$)^T$\n",
    "\n",
    "- $x$라는 대상은 $P(x)$라는 확률분포로 생성.\n",
    "??\n",
    "- 교사학습법 : $x$ 에서 $y$가 발현 - 인과성\n",
    "- 비교사학습법 : $x$에 대해서만 처리\n",
    "\n",
    "### 확률 기초\n",
    "- 곱(AND) 규칙과 합(OR) 규칙\n",
    "    \n",
    "    - 조건부 확률(conditional probability)에 의한 결합 확률 계산\n",
    "    - 곱 규칙 : Joint Probability(결합확률) - $P(y,x) = P(x|y)P(y)$\n",
    "    - ex. 카드 1번을 뽑으면서 하얀 공을 뽑을 확률 \n",
    "        - $P(y=1, x = 하양) = P(x = 하양 | y = 1) P(y = 1)$\n",
    "\n",
    "    - 합 규칙과 곱 규칙에 의한 주변확률(marginal probability) 계산\n",
    "    - 합 규칙 : $P(x) = \\sum_y P(y,x) = \\sum_y P(x|y)P(y)$\n",
    "\n",
    "\n",
    "\n",
    "- 조건부 확률 : \n",
    "\n",
    "$P(y|x) = \\frac{P(y,x)}{P(x)}$\n",
    "- 확률의 연쇄법칙 \n",
    "\n",
    "$P(x^{(1)}, ..., x^{(n)}) = P(x^{(1)}\\Pi_{i=2}^{n}P(x^{(i)}|x^{(1)}, ..., x^{(i-1)}$\n",
    "- 독립 \n",
    "\n",
    "$\\forall x \\in X, y \\in Y , P(X=x,Y=y)=P(X=x)P(Y=y)$\n",
    "- 조건부 독립 \n",
    "\n",
    " $\\forall x \\in X, y \\in Y , z \\in Z ,P(X=x,Y=y|Z=z)=P(X=x|Z=z)P(Y=y|Z=z)$\n",
    "\n",
    "- 기대값\n",
    "$E_{x~P}[f(x)] = \\displaystyle\\sum_{x}P(x)f(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이즈 정리와 기계학습\n",
    "### 베이즈 정리(Bayes's rule) \n",
    " 사후확률을 우도와 사전확률로 구하는 법\n",
    "- $P(y,x) = P(x|y)P(y) = P(x,y)$\n",
    "- $P(y|x = p) = argmax_y\\frac{P(x=p|y)P(y)}{P(x=p)}$\n",
    "- 해석\n",
    "    - $P(y|x = p)$ : 사후확률 - 어떤 일 x가 일어난 후 y가 일어날 확률 ( 기계학습 시 )\n",
    "    - $P(x=p|y)$ : 우도 - 관찰된 값에 대한 확률. 관찰할 수 있는 확률\n",
    "    - $P(y)$ : 사전확률 - 전체적인 정확한 확률값\n",
    "\n",
    "- 기계학습에 적용\n",
    "    - Ex. Iris 데이터 분류\n",
    "    - 특징벡터 $x$, class $y \\in$ (setosa, versicolor, virginica)\n",
    "    - 즉, 분류 문제 : $\\widehat y = {argmax}_yP(y|x)$\n",
    "    - 특징벡터$x$가 주어질 때, 해당 class $y$가 되는 확률을 계산하여 가장 높은 확률을 가지는 class로 분류하는 것.\n",
    "    ![Alt text](./img/image-18.png)\n",
    "    - 하지만, 사후확률 $p(y|x)$를 직접 추정하는 일은 사실상 거의 불가능함.\n",
    "    - 따라서 베이즈 정리를 이용해, 즉 사전확률과 우도를 이용해 사후확률을 추정.\n",
    "    - 우도확률은 밀도추정(densiry estimation) 기법으로 추정\n",
    "### 최대 우도\n",
    "- 어떤 확률 변수의 관찰된 값들(주어진 데이터들)을 토대로 그 확률변수와 매개변수를 구하는 방법.\n",
    "- 주어진 데이터(표본집단)가 모집단을 잘 설명해주어야 한다.\n",
    "- ex. 하얀 공이 나온 사실만 알고, 어느 병에서 나왔는지 모른다. 어느 병인지 추정하자.\n",
    "\n",
    "$\\widehat \\theta = argmax_\\theta P(X|\\theta) \n",
    "$\n",
    "\n",
    "- 수치문제를 피하기 위해 로그 표현으로 바꾸기도 한다.\n",
    "\n",
    "$\\widehat \\theta = argmax_\\theta \\log(P(X|\\theta)) = argmax_\\theta \\sum_{i=1}^n \\log(P(x_i|\\theta))\n",
    "$\n",
    "\n",
    "### 평균과 분산\n",
    "- 평균 : $\\mu = \\frac{1}{n} \\sum_{i=1}^nx_i$\n",
    "- 분산 : $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\mu)^2 \\to$\n",
    "$Var(f(x)) = E[(f(x)-E[f(x)])^2]$\n",
    "\n",
    "- 평균 벡터(치우침 정도)\n",
    "    - $\\bold\\mu = \\frac{1}{n}\\sum_{i=1}^nx_i$\n",
    "- 공분산 행렬(차원 간의 데이터의 퍼짐이나 상관도)\n",
    "    - $\n",
    "    \\begin{aligned} Σ =& \\frac{1}{n}\\sum_{i=1}^n(x_i-\\bold\\mu)(x_i-\\bold\\mu)^T\\\\\n",
    "    =& \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} & \\ cdots & \\sigma_{1d} \\\\\n",
    "    \\sigma_{21} & \\sigma_{22} & \\ cdots & \\sigma_{2d}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\sigma_{d1} & \\sigma_{d2} & \\ cdots & \\sigma_{dd}\n",
    "    \\end{pmatrix}\\\\\n",
    "    =& \\begin{pmatrix} \\sigma_1^2 & \\sigma_{12} & \\ cdots & \\sigma_{1d} \\\\\n",
    "    \\sigma_{21} & \\sigma_2^2 & \\ cdots & \\sigma_{2d}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\sigma_{d1} & \\sigma_{d2} & \\ cdots & \\sigma_d^2\n",
    "    \\end{pmatrix}\n",
    "    \\end{aligned}$\n",
    "    - $n$차원에서, $\\sigma_{ij} (1 ≤ i,j ≤ n)$은 $i$차원과 $j$차원의 분산(퍼짐의 정도)을 말한다. 이를 통해 차원간의 상관도를 알 수 있는 것. (-1~1)\n",
    "    - $\\sigma_{ii} (1 ≤ i ≤ n)$는 곧 그 차원의 분산을 말하므로 $\\sigma_{ii} = \\sigma_i^2$\n",
    "    \n",
    "\n",
    "### 가우시안 분포\n",
    "\n",
    "![Alt text](./img/image-19.png)\n",
    "\n",
    "$ N(x:µ, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp(- \\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))$\n",
    "\n",
    "- 다차원 가우시안 분포 : 평균벡터 $µ$와 공분산행렬$Σ$로 정의\n",
    "\n",
    "$ N(x:µ, ∑) = \\frac{1}{\\sigma\\sqrt{|∑|2\\pi^d}}exp(- \\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))$\n",
    "\n",
    "### 베르누이 분포\n",
    "- 성공(x=1)확률 $p$, 실패(x=0)확률 $1-p$인 분포\n",
    "\n",
    "### 이항 분포\n",
    "- 성공 확률 p인 베르누이 실험을 m번 수행할 때 성공할 횟수에 대한 분포\n",
    "\n",
    "### 확률분포와 연관된 함수들\n",
    "- 로지스틱 시그모이드 함수 : 활성함수의 대표적 예 - 베르누이 분포를 따름\n",
    "![Alt text](./img/image-15.png)\n",
    "\n",
    "\n",
    "- 소프트 플러스 함수 : 정규분포를 따름\n",
    "![Alt text](./img/image-16.png)\n",
    "\n",
    "### 지수 분포\n",
    "$P(x;\\lambda)=\\lambda 1_{x≥0}e^{-\\lambda x}$\n",
    "### 라플라스 분포\n",
    "$Laplace(x;µ,\\gamma) = \\frac{1}{2\\gamma}e^{-\\frac{|x-µ|}{\\gamma}}$\n",
    "### 디랙 분포\n",
    "$P(x) = \\delta(x-µ)$\n",
    "### 혼합 분포\n",
    "- 3개의 요소를 가진 가우시안 혼합 분포 -> 가우시안 혼합모델 추정 가능\n",
    "\n",
    "### 변수 변환\n",
    "- 기존 확률 변수를 새로운 확률변수로 바꿈\n",
    "- 변환 $v=g(x)$와 가역성을 가지는 $g$에 의해 정의되는 $x,y$두 확률변수를 가정하자. 아래와 같이 정의됨.\n",
    "![Alt text](./img/image-17.png)\n",
    "---\n",
    "- ex. 확률변수 $x$의 확률질량함수가 다음과 같다.\n",
    "\n",
    "$(\\frac{4}{5})(\\frac{1}{5})^{x-1}, (x=1, 2, ...)$\n",
    "\n",
    "새로운 확률변수 $y=x^2$의 확률질량함수는 다음과 같이 정의된다.\n",
    "\n",
    "$y=x^2 \\to x=\\sqrt{y}$\n",
    "\n",
    "$f(x)=f(\\sqrt{y})=(\\frac{4}{5})(\\frac{1}{5})^{\\sqrt{y}-1}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DevCourse_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
